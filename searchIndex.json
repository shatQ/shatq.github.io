[
{
		"title": "Backup and Restore",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/backup-and-restore/",
		"content": "Backup and Restore\nBacking up an etcd cluster\nEtcd supports built-in snapshot. A snapshot may either be taken from a live member with the etcdctl snapshot save command or by copying the member/snap/db file from an etcd data directory that is not currently used by an etcd process. Taking the snapshot will not affect the performance of the member.\nTaking a snapshot:\nETCDCTL_API=3 etcdctl --endpoints [https://127.0.0.1:2379] --cert /etc/kubernetes/pki/etcd/peer.crt --key /etc/kubernetes/pki/etcd/peer.key --cacert /etc/kubernetes/pki/etcd/ca.crtÂ snapshot save /opt/snapshot-pre-boot.db\n\nRestoring a snapshot:\nETCDCTL_API=3 etcdctl snapshot restore --data-dir=&quot;/var/lib/etcd-restored&quot; /opt/snapshot-pre-boot.db\n\nReconfiguring a cluster to use etcd data from restored location. For cluster deployed with kubeadmin edit etcd manifest file /etc/kubernetes/manifests/etcd.yaml and update volume mount path:\n- hostPath:\npath: /var/lib/etcd-restored\ntype: DirectoryOrCreate\nname: etcd-data\n\nAfter updating the file, etcd pod will be re-created and it will use restored data.\nBackup resource configs\nBacking up resource configs is method useful when we don't have access to etcd cluster, e.g. cloud managed kubernetes clusters. In such situation good option is to backup all services by querying kube-apiserver:\nkubectl get all --all-namespaces -o yaml &gt; all-deployed-services.yaml\n\nhttps://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster\nhttps://github.com/etcd-io/website/blob/main/content/en/docs/v3.5/op-guide/recovery.md",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Books",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/books/",
		"content": "Books\n\nğ—§ğ—µğ—² ğ—£ğ˜€ğ˜†ğ—°ğ—µğ—¼ğ—¹ğ—¼ğ—´ğ˜† ğ—¼ğ—³ ğ— ğ—¼ğ—»ğ—²ğ˜†, Morgan Housel\nğ—˜ğ—»ğ˜ğ—¿ğ—²ğ—½ğ—¿ğ—²ğ—»ğ—²ğ˜‚ğ—¿ ğ—¥ğ—²ğ˜ƒğ—¼ğ—¹ğ˜‚ğ˜ğ—¶ğ—¼ğ—», Daniel Priestley\nğ—§ğ—µğ—² ğ—£ğ—²ğ—¿ğ˜€ğ—¼ğ—»ğ—®ğ—¹ ğ— ğ—•A, Josh Kaufman\nğ—¢ğ˜ƒğ—²ğ—¿ğ˜€ğ˜‚ğ—¯ğ˜€ğ—°ğ—¿ğ—¶ğ—¯ğ—²ğ—±, Daniel Priestley\nğ—ªğ—µğ˜† ğ˜„ğ—² ğ˜€ğ—¹ğ—²ğ—²ğ—½, Mathew Walker\nğ—˜ğ˜€ğ˜€ğ—²ğ—»ğ˜ğ—¶ğ—®ğ—¹ğ—¶ğ˜€ğ—º, Greg McKeown\nğ—”ğ˜ğ—¼ğ—ºğ—¶ğ—° ğ—›ğ—®ğ—¯ğ—¶ğ˜ğ˜€, James Clear\nğ——ğ—¶ğ—² ğ˜„ğ—¶ğ˜ğ—µ ğ—­ğ—²ğ—¿ğ—¼, Bill Perkins\nğ—›ğ˜†ğ—½ğ—²ğ—¿ğ—³ğ—¼ğ—°ğ˜‚ğ˜€, Chris Bailey",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Cluster Upgrades",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/cluster-upgrades/",
		"content": "Cluster Upgrades\n\nKubernetes versioning convention is v&lt;major&gt;.&lt;minor&gt;.&lt;patch&gt;, e.g. v1.25.1.\nMinor versions are released every three months, whereas patch versions are released every week.\nKubernetes provides support for 3 recent minor versions.\nCore controlplane components never should be at higher version than kube-apiserver, but can be at lower version:\n\nkube-apiserver: X\ncontroller-manager: X-1\nkube-scheduler: X-1\nkublet: X-2\nkube-proxy: X-2\nkubectl: X+1 &gt;X-1\n\nWhen to Upgrade?\nUpgrade should be planned before current cluster version gots expired. Kubernetes provides support for 3 recent minor versions, e.g. once version v1.13 is being released, the v1.10 becomes unsupported. The recommended way of upgrading a cluster is one version at time. If you want upgrade from v1.10 to v1.13, you should upgrade first to v1.11, then v1.12 and finally 1.13\n%%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'showBranches': true, 'showCommitLabel':true,'mainBranchName': 'Un-Supported'}} }%%\ngitGraph\ncommit id: \"v1.10\"\ncommit id: \"v1.11\"\ncommit id: \"v1.12\"\ncommit id: \"v1.13\"\nbranch Supported\ncheckout Supported\ncommit id: \"v1.11\"\ncommit id: \"v1.12\"\ncommit id: \"v1.13\"Upgrade Strategy\nEach cluster upgrade starts from upgrading controlplane. When controlplane is down during the upgrade all pods hosted on worker nodes are not impacted.\nThere are 3 strategies for upgrading worker nodes:\n\nStrategy 1 - All at once. All worker nodes are upgraded at the same time, the incur downtime for all hosted pods.\nStrategy 2 - One by one. One worker nod is upgraded at time. Pods are evicted to other nodes for upgrade time that allows avoid downtime.\nStrategy 3 - Add new nodes. New nodes with already upgraded version are added to the cluster. Then old nodes are drained from pods and removed.\n\nUpgrade Procedure\n\nList the lates available version with kubeadm upgrade plan:\n\nroot@controlplane $ kubeadm upgrade plan\n[upgrade/config] Making sure the configuration is correct:\n[upgrade/config] Reading configuration from the cluster...\n[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[preflight] Running pre-flight checks.\n[upgrade] Running cluster health checks\n[upgrade] Fetching available versions to upgrade to\n[upgrade/versions] Cluster version: v1.26.0\n[upgrade/versions] kubeadm version: v1.26.0\nI0615 08:15:00.653385 18529 version.go:256] remote version is much newer: v1.27.3; falling back to: stable-1.26\n[upgrade/versions] Target version: v1.26.6\n[upgrade/versions] Latest version in the v1.26 series: v1.26.6\n\nComponents that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':\nCOMPONENT CURRENT TARGET\nkubelet 2 x v1.26.0 v1.26.6\n\nUpgrade to the latest version in the v1.26 series:\n\nCOMPONENT CURRENT TARGET\nkube-apiserver v1.26.0 v1.26.6\nkube-controller-manager v1.26.0 v1.26.6\nkube-scheduler v1.26.0 v1.26.6\nkube-proxy v1.26.0 v1.26.6\nCoreDNS v1.9.3 v1.9.3\netcd 3.5.6-0 3.5.6-0\n\nYou can now apply the upgrade by executing the following command:\n\nkubeadm upgrade apply v1.26.6\n\nNote: Before you can perform this upgrade, you have to update kubeadm to v1.26.6.\n\n_____________________________________________________________________\n\nThe table below shows the current state of component configs as understood by this version of kubeadm.\nConfigs that have a &quot;yes&quot; mark in the &quot;MANUAL UPGRADE REQUIRED&quot; column require manual config upgrade or\nresetting to kubeadm defaults before a successful upgrade can be performed. The version to manually\nupgrade to is denoted in the &quot;PREFERRED VERSION&quot; column.\n\nAPI GROUP CURRENT VERSION PREFERRED VERSION MANUAL UPGRADE REQUIRED\nkubeproxy.config.k8s.io v1alpha1 v1alpha1 no\nkubelet.config.k8s.io v1beta1 v1beta1 no\n_____________________________________________________________________\n\nStart from upgrading controlplane node. If the cluster was deployed with kubeadm tool, upgrade it at first place:\n\nroot@controlplane $ apt-get update &amp;&amp; apt-get install -y kubeadm=1.27.0-00\n\nIf the controlplane node is hosting pods, drain it to evict all running pods and mark the node as UnSchedulable:\n\nroot@controlplane $ kubectl drain controlplane --ignore-daemonsets\nnode/controlplane already cordoned\nWarning: ignoring DaemonSet-managed Pods: kube-flannel/kube-flannel-ds-dgwmh, kube-system/kube-proxy-27lvt\nevicting pod kube-system/coredns-787d4945fb-jwpkr\nevicting pod default/blue-987f68cb5-2qxb2\nevicting pod kube-system/coredns-787d4945fb-hxx4f\nevicting pod default/blue-987f68cb5-66qt2\npod/blue-987f68cb5-2qxb2 evicted\npod/blue-987f68cb5-66qt2 evicted\npod/coredns-787d4945fb-hxx4f evicted\npod/coredns-787d4945fb-jwpkr evicted\nnode/controlplane drained\n\nUpgrade the controlplane node:\n\nroot@controlplane $ kubeadm upgrade apply v1.27.0\n\nUpgrade kubelet and kubectl on controlplane node:\n\nroot@controlplane $ apt-get update &amp;&amp; apt-get install -y kubelet=1.27.0-00 kubectl=1.27.0-00\nroot@controlplane $ systemctl daemon-reload\nroot@controlplane $ systemctl restart kubelet\n\nMark theÂ controlplaneÂ node back to Schedulable:\n\nroot@controlplane $ kubectl uncordon controlplane\n\nDrain first worker node:\n\nroot@controlplane $ kubectl drain node01 --ignore-daemonsets\n\nUpgrade kubeadm on worker node:\n\nroot@node01 $ apt-get update &amp;&amp; apt-get install -y kubeadm=1.27.0-00\n\nUpgrade worker node:\n\nroot@node01 $ kubeadm upgrade node\n\nUpgrade kubelet and kubectl on worker node:\n\nroot@node01 $ apt-get update &amp;&amp; apt-get install -y kubelet=1.27.0-00 kubectl=1.27.0-00\nroot@node01 $ systemctl daemon-reload\nroot@node01 $ systemctl restart kubelet\n\nMark theÂ workerÂ node back to Schedulable:\n\nroot@controlplane $ kubectl uncordon node01\n\nhttps://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/\nhttps://kubernetes.io/docs/tasks/administer-cluster/kubeadm/upgrading-linux-nodes/",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Container Runtimes",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/container-runtimes/",
		"content": "Container Runtimes\n\nKubernetes was born initialy to orchestrates docker containers only.\nOver time, as more and more container runtimes were introduced, Kubernetes implemented the Container Runtime Interface (CRI) to allow use a wide variety of container runtimes.\nCRI supports all runtimes that are compatibile with Open Container Initiative (OCI)\nBecause docker was created before OCI was introduced, it was not compatibile with OCI and thus direct kubernetes integration was supported till version v1.24.\nSince v1.24 docker itegrates with kubernetes through containerD.",
		"tags": [ "note","note","seedling"]
},

{
		"title": "DNS",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/dns/",
		"content": "DNS\n\nDNS within the kubernetes cluster allows for services and pods name resolution across nodes. The DNS is served by CoreDNS service.\nService names are resolved automatically. To resolve pods, the proper option needs to be enabled in kubernetes DNS settings. Pods resolution name is pod's ip address where dots are replaced with hyphens.\nExample FQDN:\n\nHostname\nNamespace\nType\nRoot\nIP Address\n\nweb-service\napps\nsvc\ncluster.local\n10.107.37.188\n\n10-244-2-5\ndefault\npod\ncluster.local\n10.244.2.5\n\nExample search:\n\n$ curl http://web-service.apps\n$ curl http://web-service.apps.svc\n$ curl http://web-service.apps.svc.cluster.local\n\nhttps://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/\nhttps://kubernetes.io/docs/concepts/services-networking/dns-pod-service/",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Digital Garden",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/digital-garden/",
		"content": "Digital Garden\n\nExample categories:\n\nğŸŒ±Â SeedlingÂ for very rough and early ideas\nğŸŒ¿Â BuddingÂ for work I've cleaned up and clarified\nğŸŒ³Â EvergreenÂ for work that is reasonably complete (though I still tend these over time).\n\nZettelkasten\n\nFleeting Notes - Used for catching fresh ideas.\nLiterature Notes - Notes created in context of a source, e.g. while reading book. It's good if they contain information like the book title, author, chapter, and page number.\nPermanent Notes - Stand alone ideas that can be made without any direct context\n\nOther category system https://gwern.net/about#confidence-tags\n\ntopic tags\nstart and end date\nstage tag:Â draft,Â in progress, orÂ finished\ncertainty tag:Â impossible,Â unlikely,Â certain, etc.\n1-10 importance tag\n\nPublishing Obsidian notes:\n\nhttps://github.com/maximevaillancourt/digital-garden-jekyll-template\nhttps://refinedmind.co/obsidian-jekyll-workflow\n\nhttps://maggieappleton.com/garden-history\nhttps://publish.obsidian.md/bryan-jenks/Tag+Taxonomy\nhttps://zenkit.com/en/blog/a-beginners-guide-to-the-zettelkasten-method/\nhttps://haikal.blog/my-zettelkasten-journey-understanding-the-differences-between-fleeting-notes-literature-notes-reference-notes-and-permanent-notes/\nhttps://tony.alves.dev/",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Etcd",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/etcd/",
		"content": "Etcd\nCommands\nAdditional information about ETCDCTL UtilityETCDCTL is the CLI tool used to interact with ETCD.ETCDCTL can interact with ETCD Server using 2 API versions â€“ Version 2 and Version 3.Â  By default itâ€™s set to use Version 2. Each version has different sets of commands.\nFor example, ETCDCTL version 2 supports the following commands:\netcdctl backup etcdctl cluster-health etcdctl mk etcdctl mkdir etcdctl set\nWhereas the commands are different in version 3\netcdctl snapshot save etcdctl endpoint health etcdctl get etcdctl put\nTo set the right version of APIÂ set the environment variable ETCDCTL_API command\nexport ETCDCTL_API=3\nWhen the API version is not set, it is assumed to be set to version 2. And version 3 commands listed above donâ€™t work. When API version is set to version 3, version 2 commands listed above donâ€™t work.\nApart from that, you must also specify the path to certificate files so that ETCDCTL can authenticate to the ETCD API Server. The certificate files are available in the etcd-master at the following path. We discuss more about certificates in the security section of this course. So donâ€™t worry if this looks complex:\n--cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key\nSo for the commands, I showed in the previous video to work you must specify the ETCDCTL API version and path to certificate files. Below is the final form:\nkubectl exec etcd-controlplane -n kube-system -- sh -c &quot;ETCDCTL_API=3 etcdctl get / --prefix --keys-only --limit=10 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key&quot;",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Github Actions Diff Directory",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/github-actions-diff-directory/",
		"content": "Github Actions Diff Directory\nExample github action that triggers a job when change in a folder is detected:\nname: Validate terraform and check terraform file formatting\n\non: [pull_request]\n\njobs:\ngenerate_matrix:\nname: Generate matrix with modified dirs\nruns-on: ubuntu-latest\nsteps:\n- name: Checkout\nuses: actions/checkout@v3\nwith:\nfetch-depth: 2\n\n- name: Diff dirs\nid: diff-dirs\nrun: |\necho &quot;matrix=$(git --no-pager diff --name-only HEAD^ HEAD | awk -F '/' '{print $1}' | uniq | grep -E '.*-preprod-.*|.*-prod-.*' | jq -R -s -c 'split(&quot;\\n&quot;) | map(select(length &gt; 0))')&quot; &gt;&gt; $GITHUB_OUTPUT\noutputs:\nmatrix: ${{ steps.diff-dirs.outputs.matrix }}\n\nvalidate:\nname: Validate terraform\nruns-on: ubuntu-latest\nneeds: generate_matrix\nif: ${{ needs.generate_matrix.outputs.matrix != '[]' }}\nstrategy:\nmatrix:\npath: ${{ fromJson(needs.generate_matrix.outputs.matrix) }}\nsteps:\n- name: Checkout\nuses: actions/checkout@v3\n\n- name: terraform validate\nuses: dflook/terraform-validate@v1\nwith:\npath: ${{ matrix.path }}\n\ncheck_format:\nname: Check terraform formatting\nruns-on: ubuntu-latest\nneeds: generate_matrix\nif: ${{ needs.generate_matrix.outputs.matrix != '[]' }}\nstrategy:\nmatrix:\npath: ${{ fromJson(needs.generate_matrix.outputs.matrix) }}\nsteps:\n- name: Checkout\nuses: actions/checkout@v3\n\n- name: terraform fmt\nuses: dflook/terraform-fmt-check@v1\nwith:\npath: ${{ matrix.path }}\n\nvalidate_result:\nname: Validate terraform - result\nif: ${{ always() }}\nruns-on: ubuntu-latest\nneeds: validate\nsteps:\n- run: |\nvalidate=&quot;${{ needs.validate.result }}&quot;\nif <a class=\"internal-link is-unresolved\" href=\"/404\" target=\"\"> $validate == &quot;success&quot; ]] </a>; then\nexit 0\nelse\nexit 1\nfi\n\ncheck_format_result:\nname: Check terraform formatting - result\nif: ${{ always() }}\nruns-on: ubuntu-latest\nneeds: check_format\nsteps:\n- run: |\ncheck_format=&quot;${{ needs.check_format.result }}&quot;\nif <a class=\"internal-link is-unresolved\" href=\"/404\" target=\"\"> $check_format == &quot;success&quot; ]] </a>; then\nexit 0\nelse\nexit 1\nfi\n\nhttps://github.com/orgs/community/discussions/25669\nhttps://github.com/dorny/paths-filter/issues/66",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Ingress",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/ingress/",
		"content": "Ingress\nWhat is Ingress?\nIngressÂ exposes HTTP and HTTPS routes from outside the cluster toÂ servicesÂ within the cluster. Traffic routing is controlled by rules defined on the Ingress resource.\nHere is a simple example where an Ingress sends all its traffic to one Service:\n[\nAn Ingress may be configured to give Services externally-reachable URLs, load balance traffic, terminate SSL / TLS, and offer name-based virtual hosting. AnÂ Ingress controllerÂ is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic. Example ingress controllers are Nginx or HAProxy.\nAn Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically uses a service of typeÂ Service.Type=NodePortÂ orÂ Service.Type=LoadBalancer.\nCreating Ingress\nDeclarative Method\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\nname: minimal-ingress\nannotations:\nnginx.ingress.kubernetes.io/rewrite-target: /\nspec:\ningressClassName: nginx-example\nrules:\n- http:\npaths:\n- path: /testpath\npathType: Prefix\nbackend:\nservice:\nname: test\nport:\nnumber: 80\n\nImperative Method\n$ kubectl create ingress ingress-test --rule=&quot;wear.my-online-store.com/wear*=wear-service:80**\n\nAnnotations and Rewrite Target\nDifferent ingress controllers have different options that can be used to customize the way it works. NGINX Ingress controller has many options that can be seenÂ here. I would like to explain one such option that we will use in our labs. TheÂ RewriteÂ target option.\nOurÂ watchÂ app displays the video streaming webpage atÂ http://&lt;watch-service&gt;:&lt;port&gt;/\nOurÂ wearÂ app displays the apparel webpage atÂ http://&lt;wear-service&gt;:&lt;port&gt;/\nWe must configure Ingress to achieve the below. When user visits the URL on the left, his/her request should be forwarded internally to the URL on the right. Note that the /watch and /wear URL path are what we configure on the ingress controller so we can forward users to the appropriate application in the backend. The applications donâ€™t have this URL/Path configured on them:\nhttp://&lt;ingress-service&gt;:&lt;ingress-port&gt;/watchÂ â€“&gt;Â http://&lt;watch-service&gt;:&lt;port&gt;/\nhttp://&lt;ingress-service&gt;:&lt;ingress-port&gt;/wearÂ â€“&gt;Â http://&lt;wear-service&gt;:&lt;port&gt;/\nWithout theÂ rewrite-targetÂ option, this is what would happen:\nhttp://&lt;ingress-service&gt;:&lt;ingress-port&gt;/watchÂ â€“&gt;Â http://&lt;watch-service&gt;:&lt;port&gt;/watch\nhttp://&lt;ingress-service&gt;:&lt;ingress-port&gt;/wearÂ â€“&gt;Â http://&lt;wear-service&gt;:&lt;port&gt;/wear\nNoticeÂ watchÂ andÂ wearÂ at the end of the target URLs. The target applications are not configured withÂ /watchÂ orÂ /wearÂ paths. They are different applications built specifically for their purpose, so they donâ€™t expectÂ /watchÂ orÂ /wearÂ in the URLs. And as such the requests would fail and throw aÂ 404Â not found error.\nTo fix that we want to â€œReWriteâ€ the URL when the request is passed on to the watch or wear applications. We donâ€™t want to pass in the same path that user typed in. So we specify theÂ rewrite-targetÂ option. This rewrites the URL by replacing whatever is underÂ rules-&gt;http-&gt;paths-&gt;pathÂ which happens to beÂ /payÂ in this case with the value inÂ rewrite-target. This works just like a search and replace function.\nFor example:Â replace(path, rewrite-target)\nIn our case:Â replace(&quot;/path&quot;,&quot;/&quot;)\n\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\nname: test-ingress\nnamespace: critical-space\nannotations:\nnginx.ingress.kubernetes.io/rewrite-target: /\nspec:\nrules:\n- http:\npaths:\n- path: /pay\nbackend:\nserviceName: pay-service\nservicePort: 8282\n\nIn another example givenÂ here, this could also be:\nreplace(&quot;/something(/|$)(.*)&quot;, &quot;/$2&quot;)\n\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\nannotations:\nnginx.ingress.kubernetes.io/rewrite-target: /$2\nname: rewrite\nnamespace: default\nspec:\nrules:\n- host: rewrite.bar.com\nhttp:\npaths:\n- backend:\nserviceName: http-svc\nservicePort: 80\npath: /something(/|$)(.*)\n\nhttps://kubernetes.io/docs/concepts/services-networking/ingress/",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Init Containers",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/init-containers/",
		"content": "Init Containers\nIn a multi-container pod, each container is expected to run a process that stays alive as long as the PODâ€™s lifecycle. For example in the multi-container pod that we talked about earlier that has a web application and logging agent, both the containers are expected to stay alive at all times. The process running in the log agent container is expected to stay alive as long as the web application is running. If any of them fails, the POD restarts.\nBut at times you may want to run a process that runs to completion in a container. For example a process that pulls a code or binary from a repository that will be used by the main web application. That is a task that will be run onlyÂ one time when the pod is first created. Or a process that waitsÂ for an external service or database to be up before the actual application starts. Thatâ€™s whereÂ initContainersÂ comes in.\nAnÂ initContainerÂ is configured in a pod like all other containers, except that it is specified inside aÂ initContainersÂ section,Â like this:\napiVersion: v1\nkind: Pod\nmetadata:\nname: myapp-pod\nlabels:\napp: myapp\nspec:\ncontainers:\n- name: myapp-container\nimage: busybox:1.28\ncommand: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600']\ninitContainers:\n- name: init-myservice\nimage: busybox\ncommand: ['sh', '-c', 'git clone &lt;some-repository-that-will-be-used-by-application&gt;Â ;']\n\nWhen a PODÂ is first created the initContainer is run, and the process in the initContainer must run to a completion before the real container hosting the application starts.\nYou can configure multiple such initContainers as well, like how we did for multi-containers pod. In that case, each init container is runÂ one at a time in sequential order.\nIf any of the initContainers fail to complete, Kubernetes restarts the Pod repeatedly until the Init Container succeeds.\napiVersion: v1\nkind: Pod\nmetadata:\nname: myapp-pod\nlabels:\napp: myapp\nspec:\ncontainers:\n- name: myapp-container\nimage: busybox:1.28\ncommand: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600']\ninitContainers:\n- name: init-myservice\nimage: busybox:1.28\ncommand: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']\n- name: init-mydb\nimage: busybox:1.28\ncommand: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']\n\nhttps://kubernetes.io/docs/concepts/workloads/pods/init-containers/",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Install Alpine Linux on Oracle Cloud",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/install-alpine-linux-on-oracle-cloud/",
		"content": "Install Alpine Linux on Oracle Cloud\n\nCreate x86_64 instance, e.g. CentOS\nDownload the &quot;virtual&quot; type aarch64 ISO file from https://www.alpinelinux.org/downloads/ with wget\nExecute sudo dd if=alpine.iso of=/dev/sda\nOn the Oracle Cloud panel, setup a console connection and connect to the serial console.\nExecute sudo reboot\nWhen Alpine is launched and you are logged in as root, execute these commands in the serial console:mkdir /media/setup\ncp -a /media/sda/* /media/setup\nmkdir /lib/setup\ncp -a /.modloop/* /lib/setup\n/etc/init.d/modloop stop\numount /dev/sda\nmv /media/setup/* /media/sda/\nmv /lib/setup/* /.modloop/\n\nComplete the setup with setup-alpine\n\nhttps://gist.github.com/unixfox/05d661094e646947c4b303f19f9bae11\nhttps://gist.github.com/B83C/1cf528b176f171bc16d1404920093fff",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Kube-apiserver",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/kube-apiserver/",
		"content": "Kube-apiserver\nWhen you running kubectl command, e.g. to create a new pod, the following is happening:\n\nrequest is passed to kube-apiserver that:\n\nauthenticates\nvalidates request\nretrieves data\nupdates etcd\n\nkube-scheduler identifies right node to place new pod and communicates it to kube-apiserver\nkube-apiserver and passes information to kubelet on apriopriate worker node\nkubelet creates a pod and infroms kube-apiserver about the status\n\nKube-apiserve can be:\n\nrun as systemd service with a config file /etc/systemd/system/kube-apiserver.service)\nrun as a kubeadmin controled pod with a config file /etc/kubernetes/manifest/kube-apiserver.yaml\n\nreferences:",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Kube-controller-manager",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/kube-controller-manager/",
		"content": "Kube-controller-manager\nController is the process that continously monitors various components within the system and works towards bringing the whole system to the desired state. As an example:\n\nnode-controller checks status of the nodes every 5 seconds (Node Monitor Period). If it stops reciving heartbear from the node, it waits 40 seconds (Node Monitor Grace Period) until marj the node as unreachable. Then it waits 5 minutes before it starts evicting pods (Pod Eviction TImeout).\nreplication-controller that monitors replicaset to run desired number of pods.\nOther controller examples are: deployment-controller, namespace-controller, job-controller, etc.\n\nAll those controllers are combinet as a singel process named kube-controller-manager. The kube-controller-manager can be run as:\n\nrun as systemd service with a config file /etc/systemd/system/kube-controller-manager.service)\nrun as a kubeadmin controled pod with a config file /etc/kubernetes/manifest/kube-controller-manager.yaml",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Kube-proxy",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/kube-proxy/",
		"content": "Kube-proxy\nKube-proxy is process runing on each node that ensures that defined services are reachable accross the kubernetes cluster. It monitors services and for each newly created service it adds proper iptable rules.\n\nKube-proxy can be:\n\nrun as systemd service with a config file /etc/systemd/system/kube-proxy.service\nrun as a kubeadmin controled deamon-set",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Kube-scheduler",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/kube-scheduler/",
		"content": "Kube-scheduler\nThe kube-scheduler decides on witch node the pod should be running.\nKube-scheduler can be:\n\nrun as systemd service with a config file /etc/systemd/system/kube-scheduler.service\nrun as a kubeadmin controled pod with a config file /etc/kubernetes/manifest/kube-scheduler.yaml",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Kubectl Convert Plugin",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/kubectl-convert-plugin/",
		"content": "Kubectl Convert Plugin\nA plugin for Kubernetes command-line toolÂ kubectl, which allows you to convert manifests between different API versions. This can be particularly helpful to migrate manifests to a non-deprecated api version with newer Kubernetes release. For more info, visitÂ migrate to non deprecated apis\nDownload the latest release\ncurl -LO &quot;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl-convert&quot;\n\nValidate the binary (optional)\nDownload the kubectl-convert checksum file:\ncurl -LO &quot;https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl-convert.sha256&quot;\n\nValidate the kubectl-convert binary against the checksum file:\necho &quot;$(cat kubectl-convert.sha256) kubectl-convert&quot; | sha256sum --check\n\nIf valid, the output is:\nkubectl-convert: OK\n\nIf the check fails,Â sha256Â exits with nonzero status and prints output similar to:\nkubectl-convert: FAILED\nsha256sum: WARNING: 1 computed checksum did NOT match\n\n[!NOTE]Â \nDownload the same version of the binary and checksum.\n\nInstall kubectl-convert\nsudo install -o root -g root -m 0755 kubectl-convert /usr/local/bin/kubectl-convert\n\nVerify plugin is successfully installed\nkubectl convert --help\n\nIf you do not see an error, it means the plugin is successfully installed.\n\nhttps://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-kubectl-convert-plugin",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Kubectx and Kubens",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/kubectx-and-kubens/",
		"content": "Kubectx and Kubens\nKubectx\nWith this tool, you donâ€™t have to make use of lengthy â€œkubectl configâ€ commands to switch between contexts. This tool is particularly useful to switch context between clusters in a multi-cluster environment. Installation:\n$ git clone https://github.com/ahmetb/kubectx /opt/kubectx\n$ ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx\n\nKubens\nThis tool allows users to switch between namespaces quickly with a simple command. Installation:\n$ git clone https://github.com/ahmetb/kubectx /opt/kubectx\n$ ln -s /opt/kubectx/kubens /usr/local/bin/kubens",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Kubelet",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/kubelet/",
		"content": "Kubelet\nKubelet is running on each worker node and tho the following tasks:\n\nRegisters a node within kubernetes cluster\nCreate pods on kube-apiserver request\nMonitors node and pods status and reports it to kube-apiserver\n\nKublet must be always installed and run as systemd service with a config file /etc/systemd/system/kube-scheduler.service.",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Kubernetes API",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/kubernetes-api/",
		"content": "Kubernetes API\nAccessing API\n\nAPI is listening on kube-apiserver port 6433\nTo authenticate you need to pass user certificate:\n\n$ curl http://localhost:6443 -k --key admin.key --cert admin.crt --cacert ca.crt\n\nAn alternative way of authenticate is to use kubectl proxy:\n\n$ kubectl proxy\nStarting serve on 127.0.0.1:8001\n$curl http://localhost:8001 -k\n\nAPI groups\nCore group /api:\n\n/api/v1/namespaces\n/api/v1/pods\n/api/v1/rc\n/api/v1/events\n/api/v1/endpoints\n/api/v1/nodes\n/api/v1/bindings\n/api/v1/PV\n/api/v1/PVC\n/api/v1/configmaps\n/api/v1/secrets\n/api/v1/services\n\nNamed group /apis:\n\n/apps, e.g. /apps/v1/deployments\n/extensions\n/networking.k8s.io, e.g. /networking.k8s.io/v1/networkpolicies\n/storage.k8s.io\n/authentication.k8s.io\n/certificates.k8s.io, e.g. /certificates.k8s.io/v1/certificatesigningrequests",
		"tags": [ "note","note","seedling"]
},

{
		"title": "MacOS Git Push Error",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/mac-os-git-push-error/",
		"content": "MacOS Git Push Error\nfatal: Could not read from remote repository.\n\nThis error informs us we have an authentication issue. If you encounter an SSH authentication issue, your first port of call is to add your key to the SSH keychain:\nssh-add -K ~/.ssh/id_rsa\n\nThis will add our id_rsa key to the keychain.\n\nhttps://careerkarma.com/blog/git-fatal-could-not-read-from-remote-repository/",
		"tags": [ "note","note","seedling"]
},

{
		"title": "MacOS Remap Keys",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/mac-os-remap-keys/",
		"content": "MacOS Remap Keys\nRe-map right hand Command and Option keys:\nhidutil property --set '{&quot;UserKeyMapping&quot;: [{&quot;HIDKeyboardModifierMappingSrc&quot;:0x7000000e7, &quot;HIDKeyboardModifierMappingDst&quot;:0x7000000e6}, {&quot;HIDKeyboardModifierMappingSrc&quot;:0x7000000e6, &quot;HIDKeyboardModifierMappingDst&quot;:0x7000000e7}] }'\n\nTo make remap persistent create launch agent configuration file ~/Library/LaunchAgents/com.example.KeyRemapping.plist with the following content:\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;!DOCTYPE plist PUBLIC &quot;-//Apple//DTD PLIST 1.0//EN&quot; &quot;http://www.apple.com/DTDs/PropertyList-1.0.dtd&quot;&gt;\n&lt;plist version=&quot;1.0&quot;&gt;\n&lt;dict&gt;\n&lt;key&gt;Label&lt;/key&gt;\n&lt;string&gt;com.example.KeyRemapping&lt;/string&gt;\n&lt;key&gt;ProgramArguments&lt;/key&gt;\n&lt;array&gt;\n&lt;string&gt;/usr/bin/hidutil&lt;/string&gt;\n&lt;string&gt;property&lt;/string&gt;\n&lt;string&gt;--set&lt;/string&gt;\n&lt;string&gt;{&quot;UserKeyMapping&quot;:[{&quot;HIDKeyboardModifierMappingSrc&quot;:0x7000000e7, &quot;HIDKeyboardModifierMappingDst&quot;:0x7000000e6}, {&quot;HIDKeyboardModifierMappingSrc&quot;:0x7000000e6, &quot;HIDKeyboardModifierMappingDst&quot;:0x7000000e7}]}&lt;/string&gt;\n&lt;/array&gt;\n&lt;key&gt;RunAtLoad&lt;/key&gt;\n&lt;true/&gt;\n&lt;/dict&gt;\n&lt;/plist&gt;\n\nhttps://itectec.com/askdifferent/macos-changing-right-hand-command-alt-key-order-to-be-like-a-windows-keyboard/\nhttps://www.nanoant.com/mac/macos-function-key-remapping-with-hidutil\nhttps://developer.apple.com/library/archive/technotes/tn2450/_index.html",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Mapping Windows Disk with AWS EBS",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/mapping-windows-disk-with-aws-ebs/",
		"content": "Mapping Windows Disk with AWS EBS\nList NVMe disks\nThe following PowerShell script lists each disk and its corresponding device name and volume. It is intended for use with instances build on the Nitro System, which use NVMe EBS and instance store volumes.\nConnect to your Windows instance and run the following command to enable PowerShell script execution.\nSet-ExecutionPolicy RemoteSigned\n\nCopy the following script and save it as mapping.ps1 on your Windows instance.\n# List the disks for NVMe volumes\n\nfunction Get-EC2InstanceMetadata {\nparam([string]$Path)\n(Invoke-WebRequest -Uri &quot;http://169.254.169.254/latest/$Path&quot;).Content\n}\n\nfunction GetEBSVolumeId {\nparam($Path)\n$SerialNumber = (Get-Disk -Path $Path).SerialNumber\nif($SerialNumber -clike 'vol*'){\n$EbsVolumeId = $SerialNumber.Substring(0,20).Replace(&quot;vol&quot;,&quot;vol-&quot;)\n}\nelse {\n$EbsVolumeId = $SerialNumber.Substring(0,20).Replace(&quot;AWS&quot;,&quot;AWS-&quot;)\n}\nreturn $EbsVolumeId\n}\n\nfunction GetDeviceName{\nparam($EbsVolumeId)\nif($EbsVolumeId -clike 'vol*'){\n\n$Device = ((Get-EC2Volume -VolumeId $EbsVolumeId ).Attachment).Device\n$VolumeName = &quot;&quot;\n}\nelse {\n$Device = &quot;Ephemeral&quot;\n$VolumeName = &quot;Temporary Storage&quot;\n}\nReturn $Device,$VolumeName\n}\n\nfunction GetDriveLetter{\nparam($Path)\n$DiskNumber = (Get-Disk -Path $Path).Number\nif($DiskNumber -eq 0){\n$VirtualDevice = &quot;root&quot;\n$DriveLetter = &quot;C&quot;\n$PartitionNumber = (Get-Partition -DriveLetter C).PartitionNumber\n}\nelse\n{\n$VirtualDevice = &quot;N/A&quot;\n$DriveLetter = (Get-Partition -DiskNumber $DiskNumber).DriveLetter\nif(!$DriveLetter)\n{\n$DriveLetter = ((Get-Partition -DiskId $Path).AccessPaths).Split(&quot;,&quot;)[0]\n}\n$PartitionNumber = (Get-Partition -DiskId $Path).PartitionNumber\n}\n\nreturn $DriveLetter,$VirtualDevice,$PartitionNumber\n\n}\n\n$Report = @()\nforeach($Path in (Get-Disk).Path)\n{\n$Disk_ID = ( Get-Partition -DiskId $Path).DiskId\n$Disk = ( Get-Disk -Path $Path).Number\n$EbsVolumeId = GetEBSVolumeId($Path)\n$Size =(Get-Disk -Path $Path).Size\n$DriveLetter,$VirtualDevice, $Partition = (GetDriveLetter($Path))\n$Device,$VolumeName = GetDeviceName($EbsVolumeId)\n$Disk = New-Object PSObject -Property @{\nDisk = $Disk\nPartitions = $Partition\nDriveLetter = $DriveLetter\nEbsVolumeId = $EbsVolumeId\nDevice = $Device\nVirtualDevice = $VirtualDevice\nVolumeName= $VolumeName\n}\n\t$Report += $Disk\n}\n\n$Report | Sort-Object Disk | Format-Table -AutoSize -Property Disk, Partitions, DriveLetter, EbsVolumeId, Device, VirtualDevice, VolumeName\n\nList disks using PowerShell\nThe following PowerShell script lists each disk and its corresponding device name and volume.\nRequirements and limitations:\n\nRequires Windows Server 2012 or later.\nRequires credentials to get the EBS volume ID. You can configure a profile using the Tools for PowerShell, or attach an IAM role to the instance.\nDoes not support NVMe volumes.\nDoes not support dynamic disks.\n\nConnect to your Windows instance and run the following command to enable PowerShell script execution.\nSet-ExecutionPolicy RemoteSigned\n\nCopy the following script and save it as mapping.ps1 on your Windows instance.\n# List the disks\nfunction Convert-SCSITargetIdToDeviceName {\nparam([int]$SCSITargetId)\nIf ($SCSITargetId -eq 0) {\nreturn &quot;sda1&quot;\n}\n$deviceName = &quot;xvd&quot;\nIf ($SCSITargetId -gt 25) {\n$deviceName += [char](0x60 + [int]($SCSITargetId / 26))\n}\n$deviceName += [char](0x61 + $SCSITargetId % 26)\nreturn $deviceName\n}\n\n[string[]]$array1 = @()\n[string[]]$array2 = @()\n[string[]]$array3 = @()\n[string[]]$array4 = @()\n\nGet-WmiObject Win32_Volume | Select-Object Name, DeviceID | ForEach-Object {\n$array1 += $_.Name\n$array2 += $_.DeviceID\n}\n\n$i = 0\nWhile ($i -ne ($array2.Count)) {\n$array3 += ((Get-Volume -Path $array2[$i] | Get-Partition | Get-Disk).SerialNumber) -replace &quot;_[^ ]*$&quot; -replace &quot;vol&quot;, &quot;vol-&quot;\n$array4 += ((Get-Volume -Path $array2[$i] | Get-Partition | Get-Disk).FriendlyName)\n$i ++\n}\n\n[array[]]$array = $array1, $array2, $array3, $array4\n\nTry {\n$InstanceId = Get-EC2InstanceMetadata -Category &quot;InstanceId&quot;\n$Region = Get-EC2InstanceMetadata -Category &quot;Region&quot; | Select-Object -ExpandProperty SystemName\n}\nCatch {\nWrite-Host &quot;Could not access the instance Metadata using AWS Get-EC2InstanceMetadata CMDLet.\nVerify you have AWSPowershell SDK version '3.1.73.0' or greater installed and Metadata is enabled for this instance.&quot; -ForegroundColor Yellow\n}\nTry {\n$BlockDeviceMappings = (Get-EC2Instance -Region $Region -Instance $InstanceId).Instances.BlockDeviceMappings\n$VirtualDeviceMap = (Get-EC2InstanceMetadata -Category &quot;BlockDeviceMapping&quot;).GetEnumerator() | Where-Object { $_.Key -ne &quot;ami&quot; }\n}\nCatch {\nWrite-Host &quot;Could not access the AWS API, therefore, VolumeId is not available.\nVerify that you provided your access keys or assigned an IAM role with adequate permissions.&quot; -ForegroundColor Yellow\n}\n\nGet-disk | ForEach-Object {\n$DriveLetter = $null\n$VolumeName = $null\n$VirtualDevice = $null\n$DeviceName = $_.FriendlyName\n\n$DiskDrive = $_\n$Disk = $_.Number\n$Partitions = $_.NumberOfPartitions\n$EbsVolumeID = $_.SerialNumber -replace &quot;_[^ ]*$&quot; -replace &quot;vol&quot;, &quot;vol-&quot;\nif ($Partitions -ge 1) {\n$PartitionsData = Get-Partition -DiskId $_.Path\n$DriveLetter = $PartitionsData.DriveLetter | Where-object { $_ -notin @(&quot;&quot;, $null) }\n$VolumeName = (Get-PSDrive | Where-Object { $_.Name -in @($DriveLetter) }).Description | Where-object { $_ -notin @(&quot;&quot;, $null) }\n}\nIf ($DiskDrive.path -like &quot;*PROD_PVDISK*&quot;) {\n$BlockDeviceName = Convert-SCSITargetIdToDeviceName((Get-WmiObject -Class Win32_Diskdrive | Where-Object { $_.DeviceID -eq (&quot;\\\\.\\PHYSICALDRIVE&quot; + $DiskDrive.Number) }).SCSITargetId)\n$BlockDeviceName = &quot;/dev/&quot; + $BlockDeviceName\n$BlockDevice = $BlockDeviceMappings | Where-Object { $BlockDeviceName -like &quot;*&quot; + $_.DeviceName + &quot;*&quot; }\n$EbsVolumeID = $BlockDevice.Ebs.VolumeId\n$VirtualDevice = ($VirtualDeviceMap.GetEnumerator() | Where-Object { $_.Value -eq $BlockDeviceName }).Key | Select-Object -First 1\n}\nElseIf ($DiskDrive.path -like &quot;*PROD_AMAZON_EC2_NVME*&quot;) {\n$BlockDeviceName = (Get-EC2InstanceMetadata -Category &quot;BlockDeviceMapping&quot;).ephemeral((Get-WmiObject -Class Win32_Diskdrive | Where-Object { $_.DeviceID -eq (&quot;\\\\.\\PHYSICALDRIVE&quot; + $DiskDrive.Number) }).SCSIPort - 2)\n$BlockDevice = $null\n$VirtualDevice = ($VirtualDeviceMap.GetEnumerator() | Where-Object { $_.Value -eq $BlockDeviceName }).Key | Select-Object -First 1\n}\nElseIf ($DiskDrive.path -like &quot;*PROD_AMAZON*&quot;) {\nif ($DriveLetter -match '[^a-zA-Z0-9]') {\n$i = 0\nWhile ($i -ne ($array3.Count)) {\nif ($array[2][$i] -eq $EbsVolumeID) {\n$DriveLetter = $array[0][$i]\n$DeviceName = $array[3][$i]\n}\n$i ++\n}\n}\n$BlockDevice = &quot;&quot;\n$BlockDeviceName = ($BlockDeviceMappings | Where-Object { $_.ebs.VolumeId -eq $EbsVolumeID }).DeviceName\n}\nElseIf ($DiskDrive.path -like &quot;*NETAPP*&quot;) {\nif ($DriveLetter -match '[^a-zA-Z0-9]') {\n$i = 0\nWhile ($i -ne ($array3.Count)) {\nif ($array[2][$i] -eq $EbsVolumeID) {\n$DriveLetter = $array[0][$i]\n$DeviceName = $array[3][$i]\n}\n$i ++\n}\n}\n$EbsVolumeID = &quot;FSxN Volume&quot;\n$BlockDevice = &quot;&quot;\n$BlockDeviceName = ($BlockDeviceMappings | Where-Object { $_.ebs.VolumeId -eq $EbsVolumeID }).DeviceName\n}\nElse {\n$BlockDeviceName = $null\n$BlockDevice = $null\n}\nNew-Object PSObject -Property @{\nDisk = $Disk;\nPartitions = $Partitions;\nDriveLetter = If ($DriveLetter -eq $null) { &quot;N/A&quot; } Else { $DriveLetter };\nEbsVolumeId = If ($EbsVolumeID -eq $null) { &quot;N/A&quot; } Else { $EbsVolumeID };\nDevice = If ($BlockDeviceName -eq $null) { &quot;N/A&quot; } Else { $BlockDeviceName };\nVirtualDevice = If ($VirtualDevice -eq $null) { &quot;N/A&quot; } Else { $VirtualDevice };\nVolumeName = If ($VolumeName -eq $null) { &quot;N/A&quot; } Else { $VolumeName };\nDeviceName = If ($DeviceName -eq $null) { &quot;N/A&quot; } Else { $DeviceName };\n}\n} | Sort-Object Disk | Format-Table -AutoSize -Property Disk, Partitions, DriveLetter, EbsVolumeId, Device, VirtualDevice, DeviceName, VolumeName\n\nhttps://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ec2-windows-volumes.html",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Network Policies",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/network-policies/",
		"content": "Network Policies\nNetwork Policies allows to control network flow to (policyTypes: Ingress) and from (policyTypes: Egress) pods and they are attached to pods through podSelector.\nThere are four identifiers that specify what pod can communicate:\n\nports\nother pods\nnamespaces\nIP blocks with specific ports\n\nNetwork Policies are additive, means the connection allowed is the union of waht applicable pollicies allow, e.g. if there are two rules applied on a pod where one is denying connection and other is allowing it, the result will be allowed.\nExample Policy\nThe following policy is attached to all pods labeled role=db and it:\n\nallows incoming traffic on port TCP/80\nallows incoming traffic on ports TCP/6379-6380 from\n\nsubnet 172.17.0.0/16 (excluding 172.17.1.0/24)\npods in namespace labeled project=myproject\npods labeled role=frontend\n\nallows outgoing traffic on port TCP/5978 to:\n\nsubnet 10.0.0.0/24\npods labaled role: abc AND running in a namespace labaled project: xyz (Note: both destinations are defined as a single array's element)\n\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\nname: test-network-policy\nnamespace: default\nspec:\npodSelector:\nmatchLabels:\nrole: db\npolicyTypes:\n- Ingress\n- Egress\ningress:\n- ports:\n\t - protocol: TCP\n\t\t port: 80\n- from:\n- ipBlock:\ncidr: 172.17.0.0/16\nexcept:\n- 172.17.1.0/24\n- namespaceSelector:\nmatchLabels:\nproject: myproject\n- podSelector:\nmatchLabels:\nrole: frontend\n\t ports:\n- protocol: TCP\nport: 6379\nendPort: 6380\n\negress:\n- to:\n- namespaceSelector:\nmatchLabels:\nproject: xyz\npodSelector:\nmatchLabels:\nrole: abc\n- ipBlock:\ncidr: 10.0.0.0/24\nports:\n- protocol: TCP\nport: 5978\n\nDefault Policies\nBy default, if no policies exist in a namespace, then all ingress and egress traffic is allowed to and from pods in that namespace. The following examples let you change the default behavior in that namespace.\nDeny All Ingress Traffic\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\nname: default-deny-ingress\nspec:\npodSelector: {}\npolicyTypes:\n- Ingress\n\nAllow All Ingress Traffic\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\nname: allow-all-ingress\nspec:\npodSelector: {}\ningress:\n- {}\npolicyTypes:\n- Ingress\n\nDeny All Egress Traffic\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\nname: default-deny-egress\nspec:\npodSelector: {}\npolicyTypes:\n- Egress\n\nAllow All Egress Traffic\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\nname: allow-all-egress\nspec:\npodSelector: {}\negress:\n- {}\npolicyTypes:\n- Egress\n\nhttps://kubernetes.io/docs/concepts/services-networking/network-policies/",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Node Affinity",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/node-affinity/",
		"content": "Node Affinity\nNode affinity idea is similar to <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/node-selector/\">Node Selector</a> but it enables a conditional approach with logical operators in the matching process.\nNode Affinity Types\n\nrequiredDuringSchedulingIgnoredDuringExecution - Affects only scheduled pods and requires node affinity rule match, otherwise pod will not be run.\npreferredDuringSchedulingIgnoredDuringExecution - Affects only scheduled pods and prefers nodes that match affinity rule. If no match, pod will be run anyway.\nrequiredDuringSchedulingRequiredDuringExecution (planned in future release) - Affects scheduled and running pods and requires node affinity rule match. If not match, pod will not be run or if it is already running will be evicted (terminated).\n\nflowchart TD\n\tp(\"Pod nodeAffinity: tier=dev\") --> n1(\"Node1 label:tier=dev\");\n\tp -..-x n2(\"Node2 label:tier=prod\");\n\tp -..-x n3(\"Node3\");\n\t\n\tclassDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;\n\tclassDef k8s fill:#326ce5,stroke:#fff,stroke-width:0px,color:#fff;\n\tclass n1,n2,n3 k8s;\n\tclass p plain;Figure 1: Scheduling a pod with &quot;requiredDuringSchedulingIgnoredDuringExecution&quot; type. The pod will be scheduled only if there is a matching node.\nflowchart TD\n\tp(\"Pod nodeAffinity: tier=dev\") --> n1(\"Node1 label:tier=dev\");\n\tp -..-x n2(\"Node2 label:tier=prod\");\n\tp --> n3(\"Node3\");\n\t\n\tclassDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;\n\tclassDef k8s fill:#326ce5,stroke:#fff,stroke-width:0px,color:#fff;\n\tclass n1,n2,n3 k8s;\n\tclass p plain;Figure 2: Scheduling a pod with &quot;preferredDuringSchedulingIgnoredDuringExecution&quot;. The pod will be scheduled on matching node if available. Otherwise it will be scheduled on any applicable node.\nAdding Node Affinity to a Pod\napiVersion: v1\nkind: Pod\nmetadata:\nname: with-node-affinity\nspec:\naffinity:\nnodeAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\nnodeSelectorTerms:\n- matchExpressions:\n- key: topology.kubernetes.io/zone\noperator: In\nvalues:\n- antarctica-east1\n- antarctica-west1\npreferredDuringSchedulingIgnoredDuringExecution:\n- weight: 1\npreference:\nmatchExpressions:\n- key: another-node-label-key\noperator: In\nvalues:\n- another-node-label-value\ncontainers:\n- name: with-node-affinity\nimage: registry.k8s.io/pause:2.0\n\nhttps://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/\nhttps://docs.openshift.com/container-platform/4.12/nodes/scheduling/nodes-scheduler-node-affinity.html\nhttps://blog.kubecost.com/blog/kubernetes-node-affinity/",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Node Name",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/node-name/",
		"content": "Node Name\nThe nodeName is direct node selection that refers to node's name. Using nodeName overrules using nodeSelector or affinity and anti-affinity rules.\nSome of limitations:\n\nIf the named node does not exist, the Pod will not run, and in some cases may be automatically deleted.\nIf the named node does not have the resources to accommodate the Pod, the Pod will fail and its reason will indicate why, for example OutOfmemory or OutOfcpu.\nNode names in cloud environments are not always predictable or stable.\n\nAdding nodeName to a Pod\napiVersion: v1\nkind: Pod\nmetadata:\nname: nginx\nspec:\ncontainers:\n- name: nginx\nimage: nginx\nnodeName: kube-01\n\nhttps://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodename",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Node Selector",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/node-selector/",
		"content": "Node Selector\nnodeSelectorÂ is the simplest recommended form of node selection constraint. You can add theÂ nodeSelectorÂ field to your Pod specification and specify theÂ node labelsÂ you want the target node to have. Kubernetes only schedules the Pod onto nodes that have each of the labels you specify.\nLabeling a Node\nkubectl label nodes node1 tier=dev\n\nkubectl get nodes --show-labels\n\nNAME STATUS ROLES AGE VERSION LABELS\nnode1 Ready control-plane 40m v1.26.0 ...,kubernetes.io/hostname=node1,tier=dev\n\nAdding Node Selector to a Pod\napiVersion: v1\nkind: Pod\nmetadata:\nname: nginx\nlabels:\nenv: test\nspec:\nnodeSelector:\ntier: dev\ncontainers:\n- name: nginx\nimage: nginx\n\nhttps://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Private Docker Repository Credentials",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/private-docker-repository-credentials/",
		"content": "Private Docker Repository Credentials\nIf image is not specified with full path, kubernetes assumes docker hub as a default registry, e.g. nginx -&gt; docker.io/library/nginx .\nTo use image that origins from private registry, you need to specify the full path, e.g. private-registry.io/apps/internal-app.\nTo authenticate private registry, the credentials needs to be stored as secret object:\nkubectl create secret docker-registry regcred \\\n\t--docker-server=private-registry.io \\\n\t--docker-username=registry-user \\\n\t--docker-password=registry-password \\\n\t--docker-email=registry-user@org.com\n\nThen, in pod definition you need to reference image secrets in the following way:\napiVersion: v1\nkind: Pod\nmetadata:\nname: nginx-pod\nspec:\ncontainers:\n- name: nginx\nimage: private-registry.io/apps/internal-app\nimagePullSecrets:\n- name: regcred",
		"tags": [ "note","note","seedling"]
},

{
		"title": "RBAC",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/rbac/",
		"content": "RBAC\nOverview\nTo enable RBAC, start the Kubernetes API server with theÂ --authorization-modeÂ flag set.\nAn RBAC Role or ClusterRole contains rules that represent a set of permissions. Permissions are purely additive (there are no &quot;deny&quot; rules).\nA Role always sets permissions within a particular namespace; when you create a Role, you have to specify the namespace it belongs in.\nClusterRole, by contrast, is a none-namespaced resource. The resources have different names (Role and ClusterRole) because a Kubernetes object always has to be either namespaced or not namespaced. It can't be both.\nClusterRoles have several uses. You can use a ClusterRole to:\n\ndefine permissions on namespaced resources and be granted access within individual namespace(s)\ndefine permissions on namespaced resources and be granted access across all namespaces\ndefine permissions on cluster-scoped resources\n\nIf you want to define a role within a namespace, use a Role; if you want to define a role cluster-wide, use a ClusterRole.\nA RoleBinding may reference any role in the same namespace. Alternatively, a RoleBinding can reference a ClusterRole and bind that ClusterRole to the namespace defined in the RoleBinding. This kind of reference lets you define a set of common roles across your cluster, then reuse them within multiple namespaces.\nCreating a Role\nNote: For core API group specify [&quot;&quot;].\n$ vim developer-role.yaml\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\nname: developer\nrules:\n- apiGroups: [&quot;&quot;]\nresources: [&quot;pods&quot;]\nverbs: [&quot;list&quot;, &quot;get&quot;, &quot;create&quot;, &quot;update&quot;, &quot;delete&quot;]\n- apiGroups: [&quot;&quot;]\nresources: [&quot;ConfigMap&quot;]\nverbs: [&quot;create&quot;]\n\n$ kubectl create -f developer-role.yaml\n\nBinding a Role\n$ vim devuser-developer-binding.yaml\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\nname: devuser-developer-binding\nsubjects:\n- kind: User\nname: dev-user\napiGroup: rbac.authorization.k8s.io\nroleRef:\nkind: Role\nname: developer\napiGroup: rbac.authorization.k8s.io\n\n$ kubectl create -f devuser-developer-binding.yaml\n\nView RBAC\n$ kubectl get roles\n\n$ kubectl get rolebindings\n\n$ kubectl describe role developer\n\nCheck Access\n$ kubectl auth can-i create deployments\n\nCreating a ClusterRole\nTo see cluster scoped resources:\n$ kubectl api-resources --namespaced=false\n\n$ vim cluster-admin-role.yaml\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\nname: cluster-administrator\nrules:\n- apiGroups: [&quot;&quot;]\nresources: [&quot;nodes&quot;]\nverbs: [&quot;list&quot;, &quot;get&quot;, &quot;create&quot;, &quot;delete&quot;]\n\n$ kubectl create -f cluster-admin-role.yaml\n\nBinding a ClusterRole\n$ vim cluster-admin-role-binding.yaml\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\nname: cluster-admin-role-binding\nsubjects:\n- kind: User\nname: cluster-admin\napiGroup: rbac.authorization.k8s.io\nroleRef:\nkind: ClusterRole\nname: cluster-administrator\napiGroup: rbac.authorization.k8s.io\n\n$kubectl create -f cluster-admin-role-binding.yaml\n\nhttps://kubernetes.io/docs/reference/access-authn-authz/rbac/",
		"tags": [ "note","note","seedling"]
},

{
		"title": "ServiceAccounts",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/service-accounts/",
		"content": "ServiceAccounts\nOverview\nThere are two types of accounts in Kubernetes:\n\nUser - used by users\nService - used by apps\n\nCreating Service Account\n$ kubectl create serviceaccount dashboard-sa\n\n$ kubectl describe serviceaccount dashboard-sa\nName: dashboard-sa\nNamespace: default\nLabels: &lt;none&gt;\nAnnotations: &lt;none&gt;\nImage pull secrets: &lt;none&gt;\nMountable secrets: dashboard-sa-token-kbbdm\nTokens: dashboard-sa-token-kbbdm\nEvents: &lt;none&gt;\n\nSecret Token\nWhen service account is being created, kubernetes creates token and store it as a secret object. The secret object is then linked with a service account. The token needs to be exported and used as an authentication baerer token when external application calls kubernetes API.\nIf application that uses service account is located inside the cluster, there is no need of exporting the token. The secret containing the token can be simply mounted as a volume to the application pod.\nNote: v1.22 TokenRequestAPI token was introduced that is used as projected volume and mounted to the pod. The advantage is that TokenRequestAPI generated token has expiration time defined.\nNote: v1.24 Reduction of Secret-based SA Tokens. When service account is created, kubernetes no longer create secret token. To create it you need to run token command explicitly kubectl create token dashboard-sa.",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Services",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/services/",
		"content": "Services\nEach Pod gets its own IP address, however in a Deployment, the set of Pods running in one moment in time could be different from the set of Pods running that application a moment later.\nIn Kubernetes, a Service is an abstraction which defines a logical set of Pods and a policy by which to access them (sometimes this pattern is called a micro-service). The set of Pods targeted by a Service is usually determined by a selector.\nExample Service\napiVersion: v1\nkind: Service\nmetadata:\nname: my-service\nspec:\ntype: NodePort # If not specified, a default type is ClusterIP\nselector:\napp: MyApp\nports:\n- protocol: TCP\nport: 80\ntargetPort: 9376\nnodePort: 30001 # If not specified, Kubernetes control plane will allocate a port from a range (default: 30000-32767)\n\nPort Types\nService definition can specify three port types:\n\nnodePort - is a port that will be alllocated by service on each cluster node (default: 30000-32767)\nport - is a port that service exposes within a custer\ntargetPort - is a port exposed by Pod\n\nflowchart LR;\nclient([e.g. Load Balancer])-. nodePort .->node_port;\nsubgraph node [\"Node\"]\n\tnode_port[30008];\n\tsubgraph service [Service]\n\t\tport[80];\n\tend\n\tsubgraph pod1[Pod]\n\t\ttarget_port[80];\n\t\tpod1_label[app: MyApp]\n\tend\n\tsubgraph pod2[Pod]\n\t\tpod2_label[app: MyApp]\n\tend\n\tnode_port-. port .->port;\n\tservice-. targetPort .->target_port;\n\tservice-- selector --> pod1\n\tservice-- selector --> pod2\nend\n\nclassDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;\nclassDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;\nclassDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;\nclass node_port,port,target_port plain;_Figure 1: Exposing a pod on a node's port.\nService Types\nKubernetesÂ ServiceTypesÂ allow you to specify what kind of Service you want.\nTypeÂ values and their behaviors are:\n\nClusterIP: Exposes the Service on a cluster-internal IP. Choosing this value makes the Service only reachable from within the cluster. This is the default that is used if you don't explicitly specify aÂ typeÂ for a Service. You can expose the service to the public with anÂ IngressÂ or theÂ Gateway API.\nNodePort: Exposes the Service on each Node's IP at a static port (theÂ NodePort). To make the node port available, Kubernetes sets up a cluster IP address, the same as if you had requested a Service ofÂ type: ClusterIP.\nLoadBalancer: Exposes the Service externally using a cloud provider's load balancer.\nExternalName: Maps the Service to the contents of theÂ externalNameÂ field (e.g.Â foo.bar.example.com), by returning aÂ CNAMEÂ record with its value. No proxying of any kind is set up.\n\nhttps://kubernetes.io/docs/concepts/services-networking/service/",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Setting Up VIM for Yaml Editing",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/setting-up-vim-for-yaml-editing/",
		"content": "Setting Up VIM for Yaml Editing\nset expandtab\nset tabstop=2\nset shiftwidth=2\n\n:set expandtab shiftwidth=2 tabstop=2&quot;\n# Or the short form\n:set et sw=2 ts=2\n\n:set cursorcolumn\n:set cursorline\n\nhttps://www.arthurkoziel.com/setting-up-vim-for-yaml/",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Setup Dynamic Git Configs",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/setup-dynamic-git-configs/",
		"content": "Setup Dynamic Git Configs\nTo setup git config that depends on the work folder, firstly edit ~/.gitconfig and replace it's content with:\n[includeIf &quot;gitdir:~/Documents/git/personal/**/.git&quot;]\npath = .gitconfig-personal\n\n[includeIf &quot;gitdir:~/Documents/git/work/**/.git&quot;]\npath = .gitconfig-work\n\nEach section represents &quot;profile&quot; that will be used for defined workdir.\nDefined configuration for each &quot;profile&quot; as it would be reqular git config file, e.g.\ncat ~/.gitconfig-personal\n[user]\nname = Me\nemail = me@example.com\n\n[core]\nsshCommand = &quot;ssh -i ~/.ssh/me&quot;\n\n~/.gitconfig-work\n[user]\nname = Me at Work\nemail = me-at-work@work.com\n\n[core]\nsshCommand = &quot;ssh -i ~/.ssh/work&quot;\n\nhttps://gist.github.com/bgauduch/06a8c4ec2fec8fef6354afe94358c89e",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Static Pods",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/static-pods/",
		"content": "Static Pods\n\nStatic pods are created by kubelet\nThe pod yaml definition file needs to be placed under static pods path e.g. /etc/kubernetes/manifests. The designated folder path is defined in:\n\nkublet.service as a direct option `--pod-manifest-path=/etc/kubernetes/manifests\nkublet.service as indirect option --config=kubeconfig.yaml where kubeconfig.yaml defined staticPodPath:/etc/kubernetes/manifests\n\nStatic pod name is automatically appended with a node name where the pod is running, e.g. kube-apiserver-controlplane\nThe most common use case is kubeadm tool that setups cluster creating static pods for various controlplane components.",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Taints and Tolerations",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/taints-and-tolerations/",
		"content": "Taints and Tolerations\nTaints allow a node to reject a set of pods. Taint can take three effects:\n\nNoSchedule - Kubernetes will not schedule the pod onto that node.\nPreferNoSchedule - Kubernetes will try to not schedule the pod onto the node.\nNoExecute - Kubernetes will not schedule the pod onto that node and if a pod is already running on a node it will be evicted (terminated).\n\nTolerations are applied to pods. Tolerations allow the scheduler to schedule pods with matching taints. Tolerations allow scheduling but don't guarantee scheduling (see Figure 1)\nflowchart TD\n\tp(\"Pod\") --> n1(\"Node1\");\n\tp -..-x n2(\"Node2\");\n\tp --> n3(\"Node3\");\n\t\n\tclassDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;\n\tclassDef plain-green fill:#ddd,stroke:#77DD77,stroke-width:5px,color:#000;\n\tclassDef k8s fill:#326ce5,stroke:#fff,stroke-width:0px,color:#fff;\n\tclassDef k8s-green fill:#326ce5,stroke:#77DD77,stroke-width:5px,color:#fff;\n\tclassDef k8s-red fill:#326ce5,stroke:#F67280,stroke-width:5px,color:#fff;\n\tclass n1 k8s-green;\n\tclass n2 k8s-red;\n\tclass n3 k8s;\n\tclass p plain-green;_Figure 1: Scheduling of a pod with taints an tolerations. Node1 has a taint color=green:Noschedule, Node2 has color=red:Noschedule and Node3 has no taint. Pod is configured with toleration accepting color=green:NoSchedule. Scheduler can place the pod on Node1 or Node3.\nTainting a Node\nAdd taint:\nkubectl taint nodes node1 key1=value1:NoSchedule\n\nRemove taint:\nkubectl taint nodes node1 key1=value1:NoSchedule-\n\nAdding Tolerations to a Pod\napiVersion: v1\nkind: Pod\nmetadata:\nname: nginx\nlabels:\nenv: test\nspec:\ncontainers:\n- name: nginx\nimage: nginx\nimagePullPolicy: IfNotPresent\ntolerations:\n- key: &quot;key1&quot;\n\toperator: &quot;Equal&quot;\n\tvalue: &quot;value1&quot;\n\teffect: &quot;NoSchedule&quot;\n- key: &quot;key2&quot;\noperator: &quot;Exists&quot;\neffect: &quot;NoSchedule&quot;\n\nhttps://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Update EC2 User Data",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/update-ec-2-user-data/",
		"content": "Update EC2 User Data\nBy default, user data scripts and cloud-init directives run only during the first boot cycle when an EC2 instance is launched. However, you can configure your user data script and cloud-init directives with a mime multi-part file. A mime multi-part file allows your script to override how frequently user data is run in the cloud-init package. Then, the file runs the user script. The procedure:\n\nStopp the instance\nUpdate EC2 User Data with the following:\n\nContent-Type: multipart/mixed; boundary=&quot;//&quot;\nMIME-Version: 1.0\n\n--//\nContent-Type: text/cloud-config; charset=&quot;us-ascii&quot;\nMIME-Version: 1.0\nContent-Transfer-Encoding: 7bit\nContent-Disposition: attachment; filename=&quot;cloud-config.txt&quot;\n\n#cloud-config\ncloud_final_modules:\n- [scripts-user, always]\n\n--//\nContent-Type: text/x-shellscript; charset=&quot;us-ascii&quot;\nMIME-Version: 1.0\nContent-Transfer-Encoding: 7bit\nContent-Disposition: attachment; filename=&quot;userdata.txt&quot;\n\n#!/bin/bash\n/bin/echo &quot;Hello World&quot; &gt;&gt; /tmp/testfile.txt\n--//--\n\nStart the instance.\n\nhttps://repost.aws/knowledge-center/execute-user-data-ec2",
		"tags": ["cloud-config", "note","note","seedling"]
},

{
		"title": "Welcome",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/",
		"content": "Welcome\nTest",
		"tags": [ "note","gardenEntry"]
},

{
		"title": "kubectl-cheatsheet",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/garden-notes/kubectl-cheatsheet/",
		"content": "kubectl-cheatsheet\nKubectl autocomplete\nBASH\nsource &lt;(kubectl completion bash) # set up autocomplete in bash into the current shell, bash-completion package should be installed first.\necho &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; ~/.bashrc # add autocomplete permanently to your bash shell.\n\nYou can also use a shorthand alias for kubectl that also works with completion:\nalias k=kubectl\ncomplete -o default -F __start_kubectl k\n\nZSH\nsource &lt;(kubectl completion zsh) # set up autocomplete in zsh into the current shell\necho '[[ $commands[kubectl] ]] &amp;&amp; source &lt;(kubectl completion zsh)' &gt;&gt; ~/.zshrc # add autocomplete permanently to your zsh shell\n\nA note on --all-namespaces\nAppending --all-namespaces happens frequently enough where you should be aware of the shorthand for --all-namespaces:\nkubectl -A\nKubectl context and configuration\nkubectl config view # Show Merged kubeconfig settings.\n\n# use multiple kubeconfig files at the same time and view merged config\nKUBECONFIG=~/.kube/config:~/.kube/kubconfig2\n\nkubectl config get-contexts # display list of contexts\nkubectl config current-context # display the current-context\nkubectl config use-context my-cluster-name # set the default context to my-cluster-name\n\nKubectl create vs apply\n\nkubectl create\n\nIs forÂ imperative management. In this approach, you tell theÂ Kubernetes APIÂ what you want to create, replace, or delete.\nNn simpler terms,Â createÂ produces a newÂ objectÂ (previously nonexistent or deleted). If a resource already exists, it will raise an error.\n\nkubectl apply\n\nIs part of theÂ declarative managementÂ approach in which changes that you may have applied to a live object (i.e., through scale) are â€œmaintainedâ€ even if you apply other changes to the object.\napplyÂ makes incremental changes to an existing object by defining what we need. If a resource already exists, it will not raise an error.\n\nCreating objects\nKubernetes manifests can be defined in YAML or JSON. The file extension .yaml,\n.yml, and .json can be used.\nkubectl apply -f ./my-manifest.yaml # create resource(s)\nkubectl apply -f ./my1.yaml -f ./my2.yaml # create from multiple files\nkubectl apply -f ./dir # create resource(s) in all manifest files in dir\nkubectl apply -f https://git.io/vPieo # create resource(s) from url\nkubectl create deployment nginx --image=nginx # start a single instance of nginx\n\n# create a Job which prints &quot;Hello World&quot;\nkubectl create job hello --image=busybox:1.28 -- echo &quot;Hello World&quot;\n\n# create a CronJob that prints &quot;Hello World&quot; every minute\nkubectl create cronjob hello --image=busybox:1.28 --schedule=&quot;*/1 * * * *&quot; -- echo &quot;Hello World&quot;\n\nkubectl explain pods # get the documentation for pod manifests\n\nViewing and finding resources\n# Get commands with basic output\nkubectl get services # List all services in the namespace\nkubectl get pods --all-namespaces # List all pods in all namespaces\nkubectl get pods -o wide # List all pods in the current namespace, with more details\nkubectl get deployment my-dep # List a particular deployment\nkubectl get pods # List all pods in the namespace\nkubectl get pod my-pod -o yaml # Get a pod's YAML\n\n# Describe commands with verbose output\nkubectl describe nodes my-node\nkubectl describe pods my-pod\n\nUpdating resources\nkubectl set image deployment/frontend www=image:v2 # Rolling update &quot;www&quot; containers of &quot;frontend&quot; deployment, updating the image\nkubectl rollout history deployment/frontend # Check the history of deployments including the revision\nkubectl rollout undo deployment/frontend # Rollback to the previous deployment\nkubectl rollout undo deployment/frontend --to-revision=2 # Rollback to a specific revision\nkubectl rollout status -w deployment/frontend # Watch rolling update status of &quot;frontend&quot; deployment until completion\nkubectl rollout restart deployment/frontend # Rolling restart of the &quot;frontend&quot; deployment\n\ncat pod.json | kubectl replace -f - # Replace a pod based on the JSON passed into stdin\n\n# Force replace, delete and then re-create the resource. Will cause a service outage.\nkubectl replace --force -f ./pod.json\n\n# Create a service for a replicated nginx, which serves on port 80 and connects to the containers on port 8000\nkubectl expose rc nginx --port=80 --target-port=8000\n\n# Update a single-container pod's image version (tag) to v4\nkubectl get pod mypod -o yaml | sed 's/\\(image: myimage\\):.*$/\\1:v4/' | kubectl replace -f -\n\nkubectl label pods my-pod new-label=awesome # Add a Label\nkubectl label pods my-pod new-label- # Remove a label\nkubectl annotate pods my-pod icon-url=http://goo.gl/XXBTWq # Add an annotation\nkubectl autoscale deployment foo --min=2 --max=10 # Auto scale a deployment &quot;foo&quot;\n\nEditing resources\nEdit any API resource in your preferred editor.\nkubectl edit svc/docker-registry # Edit the service named docker-registry\nKUBE_EDITOR=&quot;nano&quot; kubectl edit svc/docker-registry # Use an alternative editor\n\nScaling resources\nkubectl scale --replicas=3 rs/foo # Scale a replicaset named 'foo' to 3\nkubectl scale --replicas=3 -f foo.yaml # Scale a resource specified in &quot;foo.yaml&quot; to 3\nkubectl scale --current-replicas=2 --replicas=3 deployment/mysql # If the deployment named mysql's current size is 2, scale mysql to 3\nkubectl scale --replicas=5 rc/foo rc/bar rc/baz # Scale multiple replication controllers\n\nDeleting resources\nkubectl delete -f ./pod.json # Delete a pod using the type and name specified in pod.json\nkubectl delete pod unwanted --now # Delete a pod with no grace period\nkubectl delete pod,service baz foo # Delete pods and services with same names &quot;baz&quot; and &quot;foo&quot;\nkubectl delete pods,services -l name=myLabel # Delete pods and services with label name=myLabel\nkubectl -n my-ns delete pod,svc --all # Delete all pods and services in namespace my-ns,\n\nInteracting with running Pods\nkubectl logs my-pod # dump pod logs (stdout)\nkubectl logs -l name=myLabel # dump pod logs, with label name=myLabel (stdout)\nkubectl logs my-pod --previous # dump pod logs (stdout) for a previous instantiation of a container\nkubectl logs my-pod -c my-container # dump pod container logs (stdout, multi-container case)\nkubectl logs -l name=myLabel -c my-container # dump pod logs, with label name=myLabel (stdout)\nkubectl logs my-pod -c my-container --previous # dump pod container logs (stdout, multi-container case) for a previous instantiation of a container\nkubectl logs -f my-pod # stream pod logs (stdout)\nkubectl logs -f my-pod -c my-container # stream pod container logs (stdout, multi-container case)\nkubectl logs -f -l name=myLabel --all-containers # stream all pods logs with label name=myLabel (stdout)\nkubectl run -i --tty busybox --image=busybox:1.28 -- sh # Run pod as interactive shell\nkubectl run nginx --image=nginx -n mynamespace # Start a single instance of nginx pod in the namespace of mynamespace\nkubectl run nginx --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n# Generate spec for running pod nginx and write it into a file called pod.yaml\nkubectl attach my-pod -i # Attach to Running Container\nkubectl port-forward my-pod 5000:6000 # Listen on port 5000 on the local machine and forward to port 6000 on my-pod\nkubectl exec my-pod -- ls / # Run command in existing pod (1 container case)\nkubectl exec --stdin --tty my-pod -- /bin/sh # Interactive shell access to a running pod (1 container case)\nkubectl exec my-pod -c my-container -- ls / # Run command in existing pod (multi-container case)\nkubectl top pod POD_NAME --containers # Show metrics for a given pod and its containers\nkubectl top pod POD_NAME --sort-by=cpu # Show metrics for a given pod and sort it by 'cpu' or 'memory'\n\nCopying files and directories to and from containers\nkubectl cp /tmp/foo_dir my-pod:/tmp/bar_dir # Copy /tmp/foo_dir local directory to /tmp/bar_dir in a remote pod in the current namespace\nkubectl cp /tmp/foo my-pod:/tmp/bar -c my-container # Copy /tmp/foo local file to /tmp/bar in a remote pod in a specific container\nkubectl cp /tmp/foo my-namespace/my-pod:/tmp/bar # Copy /tmp/foo local file to /tmp/bar in a remote pod in namespace my-namespace\nkubectl cp my-namespace/my-pod:/tmp/foo /tmp/bar # Copy /tmp/foo from a remote pod to /tmp/bar locally\n\nNote: kubectl cp requires that the 'tar' binary is present in your container image. If 'tar' is not present, kubectl cp will fail. For advanced use cases, such as symlinks, wildcard expansion or file mode preservation consider using kubectl exec.\ntar cf - /tmp/foo | kubectl exec -i -n my-namespace my-pod -- tar xf - -C /tmp/bar # Copy /tmp/foo local file to /tmp/bar in a remote pod in namespace my-namespace\nkubectl exec -n my-namespace my-pod -- tar cf - /tmp/foo | tar xf - -C /tmp/bar # Copy /tmp/foo from a remote pod to /tmp/bar locally\n\nInteracting with Deployments and Services\nkubectl logs deploy/my-deployment # dump Pod logs for a Deployment (single-container case)\nkubectl logs deploy/my-deployment -c my-container # dump Pod logs for a Deployment (multi-container case)\n\nkubectl port-forward svc/my-service 5000 # listen on local port 5000 and forward to port 5000 on Service backend\nkubectl port-forward svc/my-service 5000:my-service-port # listen on local port 5000 and forward to Service target port with name &lt;my-service-port&gt;\n\nkubectl port-forward deploy/my-deployment 5000:6000 # listen on local port 5000 and forward to port 6000 on a Pod created by &lt;my-deployment&gt;\nkubectl exec deploy/my-deployment -- ls # run command in first Pod and first container in Deployment (single- or multi-container cases)\n\nInteracting with Nodes and cluster\nkubectl cordon my-node # Mark my-node as unschedulable\nkubectl drain my-node # Drain my-node in preparation for maintenance\nkubectl uncordon my-node # Mark my-node as schedulable\nkubectl top node my-node # Show metrics for a given node\nkubectl cluster-info # Display addresses of the master and services\nkubectl cluster-info dump # Dump current cluster state to stdout\nkubectl cluster-info dump --output-directory=/path/to/cluster-state # Dump current cluster state to /path/to/cluster-state\n\n# If a taint with that key and effect already exists, its value is replaced as specified.\nkubectl taint nodes foo dedicated=special-user:NoSchedule\n\nResource types\nList all supported resource types along with their shortnames, API group, whether they are namespaced, and Kind:\nkubectl api-resources\n\nOther operations for exploring API resources:\nkubectl api-resources --namespaced=true # All namespaced resources\nkubectl api-resources --namespaced=false # All non-namespaced resources\nkubectl api-resources -o name # All resources with simple output (only the resource name)\nkubectl api-resources -o wide # All resources with expanded (aka &quot;wide&quot;) output\nkubectl api-resources --verbs=list,get # All resources that support the &quot;list&quot; and &quot;get&quot; request verbs\nkubectl api-resources --api-group=extensions # All resources in the &quot;extensions&quot; API group\n\nFormatting output\nTo output details to your terminal window in a specific format, add the -o (or --output) flag to a supported kubectl command.\n\nOutput format\nDescription\n\n-o=custom-columns=&lt;spec&gt;\nPrint a table using a comma separated list of custom columns\n\n-o=custom-columns-file=&lt;filename&gt;\nPrint a table using the custom columns template in the &lt;filename&gt; file\n\n-o=json\nOutput a JSON formatted API object\n\n-o=jsonpath=&lt;template&gt;\nPrint the fields defined in a jsonpath expression\n\n-o=jsonpath-file=&lt;filename&gt;\nPrint the fields defined by the jsonpath expression in the &lt;filename&gt; file\n\n-o=name\nPrint only the resource name and nothing else\n\n-o=wide\nOutput in the plain-text format with any additional information, and for pods, the node name is included\n\n-o=yaml\nOutput a YAML formatted API object\n\nExamples using -o=custom-columns:\n# All images running in a cluster\nkubectl get pods -A -o=custom-columns='DATA:spec.containers[*].image'\n\n# All images running in namespace: default, grouped by Pod\nkubectl get pods --namespace default --output=custom-columns=&quot;NAME:.metadata.name,IMAGE:.spec.containers[*].image&quot;\n\n# All images excluding &quot;registry.k8s.io/coredns:1.6.2&quot;\nkubectl get pods -A -o=custom-columns='DATA:spec.containers[?(@.image!=&quot;registry.k8s.io/coredns:1.6.2&quot;)].image'\n\n# All fields under metadata regardless of name\nkubectl get pods -A -o=custom-columns='DATA:metadata.*'\n\nKubectl output verbosity and debugging\nKubectl verbosity is controlled with the -v or --v flags followed by an integer representing the log level. General Kubernetes logging conventions and the associated log levels are described here.\n\nVerbosity\nDescription\n\n--v=0\nGenerally useful for this to always be visible to a cluster operator.\n\n--v=1\nA reasonable default log level if you don't want verbosity.\n\n--v=2\nUseful steady state information about the service and important log messages that may correlate to significant changes in the system. This is the recommended default log level for most systems.\n\n--v=3\nExtended information about changes.\n\n--v=4\nDebug level verbosity.\n\n--v=5\nTrace level verbosity.\n\n--v=6\nDisplay requested resources.\n\n--v=7\nDisplay HTTP request headers.\n\n--v=8\nDisplay HTTP request contents.\n\n--v=9\nDisplay HTTP request contents without truncation of contents.\n\nhttps://www.containiq.com/post/kubectl-apply-vs-create\nhttps://kubernetes.io/docs/reference/kubectl/cheatsheet/\nhttps://github.com/dennyzhang/cheatsheet-kubernetes-A4",
		"tags": [ "note","note","seedling"]
},

{
		"title": "Kubernetes MOC",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/mo-cs/kubernetes-moc/",
		"content": "Kubernetes MOC\nArchitecture\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/etcd/\">Etcd</a> - It is a strongly consistent, distributed key-value store that stores information about the cluster.\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/kube-scheduler/\">Kube-scheduler</a> - Responsible for scheduling application containers on worker nodes\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/kube-controller-manager/\">Kube-controller-manager</a> - Take care of different cluster functions, e.g. node controller, replication controller, etc.\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/kube-apiserver/\">Kube-apiserver</a> - Responsible for orchestrating all operations within the cluster.\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/kubelet/\">Kubelet</a> - Listens for instructions from kube-apiserver and manages containers.\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/kube-proxy/\">Kube-proxy</a> - Helps enabling communication between services within the cluster.\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/container-runtimes/\">Container Runtimes</a>\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/kubernetes-api/\">Kubernetes API</a>\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/static-pods/\">Static Pods</a>\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/init-containers/\">Init Containers</a>\n\nSecurity\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/rbac/\">RBAC</a>\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/service-accounts/\">ServiceAccounts</a>\n\nNetwork\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/dns/\">DNS</a>\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/ingress/\">Ingress</a>\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/network-policies/\">Network Policies</a>\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/services/\">Services</a>\n\nScheduling\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/node-affinity/\">Node Affinity</a>\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/node-selector/\">Node Selector</a>\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/node-name/\">Node Name</a>\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/taints-and-tolerations/\">Taints and Tolerations</a>\n\nMaintenance\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/cluster-upgrades/\">Cluster Upgrades</a>\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/backup-and-restore/\">Backup and Restore</a>\n\nTools\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/kubectl-cheatsheet/\">kubectl-cheatsheet</a>\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/kubectx-and-kubens/\">Kubectx and Kubens</a>\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/kubectl-convert-plugin/\">Kubectl Convert Plugin</a>\n\nOther\n\n<a class=\"internal-link is-unresolved\" href=\"/404\" target=\"\">CKAD Simulator Kubernetes 1.26</a>",
		"tags": [ "note","moc"]
},

{
		"title": "aws",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/topics/aws/",
		"content": "aws\n\nFile\nLast Modified\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/update-ec-2-user-data/\">Update EC2 User Data</a>\n2:52 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/mapping-windows-disk-with-aws-ebs/\">Mapping Windows Disk with AWS EBS</a>\n2:51 PM - November 29, 2024",
		"tags": [ "note","topic"]
},

{
		"title": "git",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/topics/git/",
		"content": "git\n\nFile\nLast Modified\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/setup-dynamic-git-configs/\">Setup Dynamic Git Configs</a>\n2:52 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/mac-os-git-push-error/\">MacOS Git Push Error</a>\n2:51 PM - November 29, 2024",
		"tags": [ "note","topic"]
},

{
		"title": "k8s",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/topics/k8s/",
		"content": "k8s\n\nFile\nLast Modified\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/init-containers/\">Init Containers</a>\n9:32 AM - November 30, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/update-ec-2-user-data/\">Update EC2 User Data</a>\n2:52 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/taints-and-tolerations/\">Taints and Tolerations</a>\n2:52 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/static-pods/\">Static Pods</a>\n2:52 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/services/\">Services</a>\n2:52 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/service-accounts/\">ServiceAccounts</a>\n2:52 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/rbac/\">RBAC</a>\n2:52 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/private-docker-repository-credentials/\">Private Docker Repository Credentials</a>\n2:51 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/node-selector/\">Node Selector</a>\n2:51 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/node-name/\">Node Name</a>\n2:51 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/node-affinity/\">Node Affinity</a>\n2:51 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/network-policies/\">Network Policies</a>\n2:51 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/kubernetes-api/\">Kubernetes API</a>\n2:51 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/kubelet/\">Kubelet</a>\n2:51 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/kubectx-and-kubens/\">Kubectx and Kubens</a>\n2:51 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/kubectl-cheatsheet/\">kubectl-cheatsheet</a>\n2:51 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/kubectl-convert-plugin/\">Kubectl Convert Plugin</a>\n2:51 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/kube-scheduler/\">Kube-scheduler</a>\n2:50 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/kube-proxy/\">Kube-proxy</a>\n2:50 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/kube-controller-manager/\">Kube-controller-manager</a>\n2:50 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/kube-apiserver/\">Kube-apiserver</a>\n2:50 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/ingress/\">Ingress</a>\n2:50 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/etcd/\">Etcd</a>\n2:50 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/dns/\">DNS</a>\n2:50 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/container-runtimes/\">Container Runtimes</a>\n2:43 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/cluster-upgrades/\">Cluster Upgrades</a>\n2:43 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/backup-and-restore/\">Backup and Restore</a>\n2:43 PM - November 29, 2024",
		"tags": [ "note","topic"]
},

{
		"title": "macos",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/topics/macos/",
		"content": "macos\n\nFile\nLast Modified\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/mac-os-remap-keys/\">MacOS Remap Keys</a>\n2:51 PM - November 29, 2024\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/garden-notes/mac-os-git-push-error/\">MacOS Git Push Error</a>\n2:51 PM - November 29, 2024",
		"tags": [ "note","topic"]
},

{
		"title": "redhat",
		"date":"Tue Dec 03 2024 13:35:22 GMT+0000 (Coordinated Universal Time)",
		"url":"/topics/redhat/",
		"content": "redhat\n\nFile\nLast Modified",
		"tags": [ "note","topic"]
}
]